% TODO: only enable `screen` for web review
\documentclass[format=sigconf]{acmart}
\usepackage{geometry}
\geometry{a4paper}
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
%\usepackage{graphicx}
%\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{epstopdf}
\usepackage[utf8]{inputenc}

\usepackage{tikz}
\usetikzlibrary{arrows, calc, shapes}
\usepackage{datastore}

\settopmatter{printacmref=false}
\setcopyright{none}

%\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\bibliographystyle{acm}

\title{Collaborative auditing: a challenge to secrecy in artificial intelligence}
\author{Adam Roses Wight}
\email{awight@wikimedia.org}
\affiliation{Wikimedia Foundation}
%\date{}                                           % Activate to display a given date or no date

\acmConference[WebSci'18]{Understanding the political economy of digital technology}{May 2018}{Amsterdam, NL}
\acmISBN{}
\acmDOI{}

\begin{document}

\begin{abstract}
We introduce a system for detecting and mitigating bias in artificial intelligence, called Judgment and Dialogue Engine (JADE).  Expanding on current auditing approaches, JADE adds a dimension of human communication and collaborative decision-making between the auditors.

Collaborative auditing challenges the hegemony of opaque AIs, offering a platform for holding algorithms accountable and building community among AI investigators.  Audit results will be analyzed and fed back as AI training data in order to iteratively uncover and mitigate biases.  Our hope is that JADE will improve AI fairness and performance, and may help establish transparent and collaborative auditing as an urgent intervention to make in the general interest.
\end{abstract}

\maketitle

\section{Introduction}

Artificial intelligence has become indispensable to the largest digital businesses, from search engines and email hosts, to shopping, mortgage, insurance, and law enforcement services.  AI is used to provide quality control, curation, and analytics at massive scales, rather than having humans do the work.

Human decision-making has been replaced by its emulation, interpolated from previously recorded decisions, and during this process is heavily mediated by algorithm owners and designers.  These last two categories are the elite of the technocratic hierarchy, and although every rung of that ladder shares some responsibility, they all go largely unaccountable for the quality and social impact of the AIs they deploy.  In commercial settings, the only constant, guiding force in the absence of other constraints must be the profit motive.\footnote{See Dodge v. Ford (1919) for the corporate mandate in a nutshell.}  It will take a fight to bring this industry to account.

Corporate ownership of powerful AIs which affect people's life chances (mortgages, law enforcement) and exacerbate existing social ills (redlining, racial profiling) is beyond troubling, because we in the USA have no oversight or democratic accountability by which we can fight back yet.  AIs operated by companies or governments are closed by default.  Companies rely on this information disparity, hiding secrets to protect profit margins, and have shown no interest in public review of algorithms or data.

All AIs learn from humans, and will replicate our prejudices or group polarization.  Just as it's difficult for humans to be self-critical and diagnose our own prejudices, AIs are blind to their own built-in biases and cannot give an accurate reckoning, and owners hardly ever admit to side-effects they become aware of.\footnote{Solon Barocas (2014) \citep{barocas2014data} gives an overview of the types of bias and in section 2.5 demonstrates a positive feedback loop between AI predictions and iterations on the sample frame.}  Playing with fire, in the dark and uncertain of the consequences, AI practitioners really are Ali Rahimi's modern alchemists.\footnote{\url{http://www.argmin.net/2017/12/05/kitchen-sinks/}}

Exploring and measuring biases is imperative, and researchers have developed methods which will serve even without privileged access to AI internals.  Christian Sandvig (2014) \citep{Sandvig} illustrates these methods, but also warns that high-quality information gathering for an independent audit is prohibited by most sites' terms of service, and can even trigger felonies under the United States' Computer Fraud and Abuse Act.

For the moment, we leave it to AI engineers to audit their own systems.  The emerging industry standard for user auditing feedback falls well behind the state of the art for rich feedback\footnote{Stephanie Rosenthal and Anind Dey (2010) \citep{Rosenthal2010} optimize the choice of data to include in rich feedback.}, and will present some sample of users with a dull feedback dialog, posing a question like "Do you agree with this recommendation?".  In these systems, the respondent may have the option to leave a comment, but that's the beginning and end of any interaction.\footnote{For example, Google Ideas's Perspective API \url{https://github.com/conversationai/perspectiveapi/blob/master/api_reference.md\#sending-feedback-suggestcommentscore} is ahead of the curve only because it allows a client response to include rationales as in Sharma (2015) \citep{Sharma2015}.}  Reviewers are alienated from one another, cannot read or discuss other users' responses, and have no agency.  They are reduced to mere processes, gathering and corroborating data in silent redundancy.

\begin{figure}
\begin{tikzpicture}[
  font=\sffamily,
  every matrix/.style={ampersand replacement=\&, column sep=2cm, row sep=2cm},
  interface/.style={draw, thick, regular polygon, regular polygon sides=4, inner sep=0},
  process/.style={draw, thick, rounded corners, inner sep=.3 cm},
  datastore/.style={draw, thick, shape=datastore, inner sep=.3cm},
  to/.style={->, >=stealth', shorten >=1pt, semithick, font=\sffamily\footnotesize},
  both/.style={<->, >=stealth', shorten >=1pt, semithick, font=\sffamily\footnotesize},
  every node/.style={align=center}]

  % Position the nodes using a matrix layout
  \matrix{
    \node[process] (editor) {Contributors};
      \& \node[datastore] (content) {Content}; \\
    \node[datastore] (scores) {Machine \\ predictions};
      \& \node[process] (ai) {Artificial \\ intelligence}; \\
    \node [process] (patroller) {Reviewers};
      \& \node [datastore] (judgments) {Judgments}; \\
  };

  % Place area labels at center of boxes.
  \node [align=flush center] at ($(content)!0.5!(scores)$) {\Large{conformity} \\ \Large{cycle}};
  \node [align=flush center] at ($(ai)!0.5!(patroller)$) {\Large{collaborative} \\ \Large{auditing}};

  % Draw the arrows between the nodes and label them.
  \draw[to] (content) to[bend left=25] node[midway, right] {articles, \\ edits} (ai);
  \draw[to] (ai) -- node[midway, above] {raw scores} (scores);
  \draw[to] (scores) to[bend left=25] node[midway, left] {rendered \\ scores} (editor);
  \draw[to] (scores) to[bend right=25] node[midway, left] {scores, \\ justifications} (patroller);

  % edits: Two-way cycle with label between legs.
  \draw[to] (editor) to[bend left=25] node[midway, above] {} (content);
  \draw[to] (content) to[bend left=25] node[midway, above] {} (editor);
  \node [align=flush center] at ($(editor)!0.5!(content)$) {edits, \\ curation};

  % judgments: Two-way cycle with label between legs.
  \draw[to] (patroller) to[bend left=25] node[midway, above] {} (judgments);
  \draw[to, dashed] (judgments) to[bend left=25] node[midway, above] {} (patroller);
  \node [align=flush center] at ($(patroller)!0.5!(judgments)$) {judgments, \\ rationales};

  \draw[to] (judgments) to[bend right=25] node[midway, right] {false positives, \\ training data} (ai);
  
  \draw[to] (editor) to[loop, out=60, in=120, looseness=5] node[near end, left] {discussion, \\ consensus} (editor);

  \draw[to, dashed] (patroller) to[loop, out=-60, in=-120, looseness=5] node[near end, left] {discussion, \\ consensus} (patroller);

\end{tikzpicture}
\caption{AI feedback with auditing, in Wikipedia.  Dashed lines are the data flows unique to JADE.}\label{fig:audited}
\end{figure}

\section{Judgment and Dialogue Engine}

We're introducing Judgment and Dialogue Engine in order to audit the AIs used on Wikipedia and its sister sites, and to humanize this process through transparency and consensus, already strong traditions among these communities.  In figure~\ref{fig:audited}, JADE is the lower feedback loop involving reviewers, shown here in relation to the upper, "conformity cycle" feedback loop in which AI helps to confirm what editors already believe.

In practice, reviewers using JADE will be reading through wiki content and looking at machine predictions, and will begin a JADE session either to flag an incorrect prediction, or simply to record their judgments about wiki content.  JADE will provide open access to the data created by its reviewers.  Crucially, reviewers are able to read one another's judgments, discuss any disagreements, and make changes to the recorded judgment.

The best outcome would be a disintermediation that takes power away from the algorithm designers and gives it to the reviewers.  In this scenario, the data from reviewers would feed back into AI training without mediation by AI technicians.  The worst outcome would be that we enable the formation of a small group which undergoes polarization, and pushes the AI toward even deeper biases.

If our society can agree that AIs must be regulated by a "right to audit"\footnote{A precedent for this sort of regulation is the U.S. Federal Aviation Administration's Aircraft Certification Service, which reviews and certifies all software and hardware to be used in aircraft.}, then JADE may serve as an example for how to accomplish this auditing in a pro-social environment.

\bibliography{references}

\end{document}
